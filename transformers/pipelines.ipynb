{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417a9e80-e1be-4db4-bfa5-831570a39fe3",
   "metadata": {},
   "source": [
    "# HF Transformers 核心模块学习：Pipelines\n",
    "\n",
    "**Pipelines**（管道）是使用模型进行推理的一种简单易上手的方式。\n",
    "\n",
    "这些管道是抽象了 Transformers 库中大部分复杂代码的对象，提供了一个专门用于多种任务的简单API，包括**命名实体识别、掩码语言建模、情感分析、特征提取和问答**等。\n",
    "\n",
    "\n",
    "| Modality                    | Task                         | Description                                                | Pipeline API                                  |\n",
    "| --------------------------- | ---------------------------- | ---------------------------------------------------------- | --------------------------------------------- |\n",
    "| Audio                       | Audio classification         | 为音频文件分配一个标签                                     | pipeline(task=“audio-classification”)         |\n",
    "|                             | Automatic speech recognition | 将音频文件中的语音提取为文本                               | pipeline(task=“automatic-speech-recognition”) |\n",
    "| Computer vision             | Image classification         | 为图像分配一个标签                                         | pipeline(task=“image-classification”)         |\n",
    "|                             | Object detection             | 预测图像中目标对象的边界框和类别                           | pipeline(task=“object-detection”)             |\n",
    "|                             | Image segmentation           | 为图像中每个独立的像素分配标签（支持语义、全景和实例分割） | pipeline(task=“image-segmentation”)           |\n",
    "| Natural language processing | Text classification          | 为给定的文本序列分配一个标签                               | pipeline(task=“sentiment-analysis”)           |\n",
    "|                             | Token classification         | 为序列里的每个 token 分配一个标签（人, 组织, 地址等等）    | pipeline(task=“ner”)                          |\n",
    "|                             | Question answering           | 通过给定的上下文和问题, 在文本中提取答案                   | pipeline(task=“question-answering”)           |\n",
    "|                             | Summarization                | 为文本序列或文档生成总结                                   | pipeline(task=“summarization”)                |\n",
    "|                             | Translation                  | 将文本从一种语言翻译为另一种语言                           | pipeline(task=“translation”)                  |\n",
    "| Multimodal                  | Document question answering  | 根据给定的文档和问题回答一个关于该文档的问题。             | pipeline(task=“document-question-answering”)  |\n",
    "|                             | Visual Question Answering    | 给定一个图像和一个问题，正确地回答有关图像的问题           | pipeline(task=“vqa”)                          |\n",
    "\n",
    "\n",
    "\n",
    "Pipelines 已支持的完整任务列表：https://huggingface.co/docs/transformers/task_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f2e3c-a185-4164-ab8b-3b8c98c3b4a5",
   "metadata": {},
   "source": [
    "## Pipeline API\n",
    "\n",
    "**Pipeline API** 是对所有其他可用管道的包装。它可以像任何其他管道一样实例化，并且降低AI推理的学习和使用成本。\n",
    "\n",
    "![](docs/images/pipeline_func.png)\n",
    "\n",
    "### 使用 Pipeline API 实现 Text Classification 任务\n",
    "\n",
    "\n",
    "**Text classification**(文本分类)与任何模态中的分类任务一样，文本分类将一个文本序列（可以是句子级别、段落或者整篇文章）标记为预定义的类别集合之一。文本分类有许多实际应用，其中包括：\n",
    "\n",
    "- 情感分析：根据某种极性（如积极或消极）对文本进行标记，以在政治、金融和市场等领域支持决策制定。\n",
    "- 内容分类：根据某个主题对文本进行标记，以帮助组织和过滤新闻和社交媒体信息流中的信息（天气、体育、金融等）。\n",
    "\n",
    "\n",
    "下面以 `Text classification` 中的情感分析任务为例，展示如何使用 Pipeline API。\n",
    "\n",
    "模型主页：https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## transformers 自定义模型下载的路径\n",
    "\n",
    "在transformers自定义模型下载的路径方法\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = '/mnt/new_volume/hf'\n",
    "os.environ['HF_HUB_CACHE'] = '/mnt/new_volume/hf/hub'\n",
    "```"
   ],
   "metadata": {
    "tags": []
   },
   "id": "b9c38e82-e8f6-4af0-b257-b46f446aa249"
  },
  {
   "cell_type": "markdown",
   "id": "63a12486-952a-447d-9e90-6b041349a26e",
   "metadata": {},
   "source": [
    "### 测试更多示例"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'negative', 'score': 0.8376048803329468}]"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 仅指定任务时，使用默认模型（不推荐）\n",
    "# pipe = pipeline(\"sentiment-analysis\")\n",
    "pipe = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "pipe(\"今儿上海可真冷啊\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:45:46.089302862Z",
     "start_time": "2024-02-15T10:45:43.881754724Z"
    }
   },
   "id": "9684ec5f-9460-4876-9883-69380eacb0e7",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'negative', 'score': 0.45757803320884705}]"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"我觉得这家店蒜泥白肉的味道一般\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:45:48.942989729Z",
     "start_time": "2024-02-15T10:45:47.589475735Z"
    }
   },
   "id": "ee67f529-3d32-4834-b29a-e51a65cf4d49",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'positive', 'score': 0.6454275846481323}]"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认使用的模型 distilbert-base-uncased-finetuned-sst-2-english \n",
    "# 并未针对中文做太多训练，中文的文本分类任务表现未必满意\n",
    "pipe(\"你学东西真的好快，理论课一讲就明白了\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:45:52.697557717Z",
     "start_time": "2024-02-15T10:45:50.793564727Z"
    }
   },
   "id": "4e104034-94ca-4370-8d35-438501c29ead",
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'positive', 'score': 0.503150224685669}]"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 替换为英文后，文本分类任务的表现立刻改善\n",
    "pipe(\"You learn things really quickly. You understand the theory class as soon as it is taught.\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:45:58.233704761Z",
     "start_time": "2024-02-15T10:45:56.720699976Z"
    }
   },
   "id": "9c3d11aa-d523-491c-8910-bdc8aba49487",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09df7300-2f0d-4bc4-9572-0631658e8253",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:46:03.913957992Z",
     "start_time": "2024-02-15T10:46:02.862587801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'negative', 'score': 0.8986711502075195}]"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Today Shanghai is really cold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507dbfc3-5347-4b82-8ea1-4e6c3d81c07f",
   "metadata": {},
   "source": [
    "### 批处理调用模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1390396e-1af8-497f-85d9-6c9e984b4609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:46:12.745024594Z",
     "start_time": "2024-02-15T10:46:08.695599593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'negative', 'score': 0.8986711502075195},\n {'label': 'negative', 'score': 0.8734230995178223},\n {'label': 'positive', 'score': 0.503150224685669}]"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"Today Shanghai is really cold.\",\n",
    "    \"I think the taste of the garlic mashed pork in this store is average.\",\n",
    "    \"You learn things really quickly. You understand the theory class as soon as it is taught.\"\n",
    "]\n",
    "\n",
    "pipe(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3686758b-d7f7-4df9-889d-e63af47a138a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T09:13:20.214606279Z",
     "start_time": "2024-02-15T09:13:20.212316580Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d5a27fe-87d0-45fc-a31a-9a8db23e290a",
   "metadata": {},
   "source": [
    "## 使用 Pipeline API 调用更多预定义任务\n",
    "\n",
    "## Natural Language Processing(NLP)\n",
    "\n",
    "**NLP**(自然语言处理)任务是最常见的任务类型之一，因为文本是我们进行交流的一种自然方式。要将文本转换为模型可识别的格式，需要对其进行分词。这意味着将一系列文本划分为单独的单词或子词（标记），然后将这些标记转换为数字。结果就是，您可以将一系列文本表示为一系列数字，并且一旦您拥有了一系列数字，它就可以输入到模型中来解决各种NLP任务！\n",
    "\n",
    "上面演示的 文本分类任务，以及接下来的标记、问答等任务都属于 NLP 范畴。\n",
    "\n",
    "### Token Classification\n",
    "\n",
    "在任何NLP任务中，文本都经过预处理，将文本序列分成单个单词或子词。这些被称为tokens。\n",
    "\n",
    "**Token Classification**（Token分类）将每个token分配一个来自预定义类别集的标签。\n",
    "\n",
    "两种常见的 Token 分类是：\n",
    "\n",
    "- 命名实体识别（NER）：根据实体类别（如组织、人员、位置或日期）对token进行标记。NER在生物医学设置中特别受欢迎，可以标记基因、蛋白质和药物名称。\n",
    "- 词性标注（POS）：根据其词性（如名词、动词或形容词）对标记进行标记。POS对于帮助翻译系统了解两个相同的单词如何在语法上不同很有用（作为名词的银行与作为动词的银行）。\n",
    "\n",
    "模型主页：https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f1ed125-f9ed-42d9-b102-dbc3172baefd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:52:18.289619006Z",
     "start_time": "2024-02-15T10:50:45.445729922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "881b9de5b65242b9aa9487e720f3d54c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c2eb07cb0bf4f04a73a4deff6d5defa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "acb3aacaf2ca4884952ad928eb31c9b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "deb5910e06094023af6d9f7f3230c835"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be55c3a5e71f4fc59a9372dcb3870bad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6a4330c253144cb834b0c59571cd4a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "added_tokens.json:   0%|          | 0.00/16.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7a672dfe5164eb9918832375e3646a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ab94dbc772944b0bb9174517b9857dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# classifier = pipeline(\"ner\")\n",
    "classifier = pipeline(task=\"ner\", model=\"brad1141/GPT2_v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68bab77c-0fe6-4781-b978-02d56829db33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:53:05.568896368Z",
     "start_time": "2024-02-15T10:53:03.398709997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'Lead', 'score': 0.5913, 'index': 0, 'word': 'ĠHug', 'start': 0, 'end': 3}\n",
      "{'entity': 'Lead', 'score': 0.6609, 'index': 1, 'word': 'ging', 'start': 3, 'end': 7}\n",
      "{'entity': 'Lead', 'score': 0.519, 'index': 2, 'word': 'ĠFace', 'start': 7, 'end': 12}\n",
      "{'entity': 'Position', 'score': 0.5123, 'index': 3, 'word': 'Ġis', 'start': 12, 'end': 15}\n",
      "{'entity': 'Position', 'score': 0.5247, 'index': 4, 'word': 'Ġa', 'start': 15, 'end': 17}\n",
      "{'entity': 'Evidence', 'score': 0.702, 'index': 5, 'word': 'ĠFrench', 'start': 17, 'end': 24}\n",
      "{'entity': 'Evidence', 'score': 0.3731, 'index': 6, 'word': 'Ġcompany', 'start': 24, 'end': 32}\n",
      "{'entity': 'Evidence', 'score': 0.5943, 'index': 7, 'word': 'Ġbased', 'start': 32, 'end': 38}\n",
      "{'entity': 'Evidence', 'score': 0.6918, 'index': 8, 'word': 'Ġin', 'start': 38, 'end': 41}\n",
      "{'entity': 'Evidence', 'score': 0.7792, 'index': 9, 'word': 'ĠNew', 'start': 41, 'end': 45}\n",
      "{'entity': 'Evidence', 'score': 0.6701, 'index': 10, 'word': 'ĠYork', 'start': 45, 'end': 50}\n",
      "{'entity': 'Evidence', 'score': 0.565, 'index': 11, 'word': 'ĠCity', 'start': 50, 'end': 55}\n",
      "{'entity': 'Lead', 'score': 0.5266, 'index': 12, 'word': '.', 'start': 55, 'end': 56}\n"
     ]
    }
   ],
   "source": [
    "preds = classifier(\"Hugging Face is a French company based in New York City.\")\n",
    "preds = [\n",
    "    {\n",
    "        \"entity\": pred[\"entity\"],\n",
    "        \"score\": round(pred[\"score\"], 4),\n",
    "        \"index\": pred[\"index\"],\n",
    "        \"word\": pred[\"word\"],\n",
    "        \"start\": pred[\"start\"],\n",
    "        \"end\": pred[\"end\"],\n",
    "    }\n",
    "    for pred in preds\n",
    "]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c72d55-e574-444f-96bc-62704022a148",
   "metadata": {},
   "source": [
    "#### 合并实体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9b441191-6156-44b8-a323-db461ad06efb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:53:16.258931251Z",
     "start_time": "2024-02-15T10:53:13.999297279Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'entity_group': 'Lead',\n  'score': 0.59042567,\n  'word': ' Hugging Face',\n  'start': 0,\n  'end': 12},\n {'entity_group': 'Position',\n  'score': 0.5185073,\n  'word': ' is a',\n  'start': 12,\n  'end': 17},\n {'entity_group': 'Evidence',\n  'score': 0.6250814,\n  'word': ' French company based in New York City',\n  'start': 17,\n  'end': 55},\n {'entity_group': 'Lead',\n  'score': 0.5265786,\n  'word': '.',\n  'start': 55,\n  'end': 56}]"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier = pipeline(task=\"ner\", grouped_entities=True)\n",
    "classifier = pipeline(task=\"ner\", model=\"brad1141/GPT2_v5\", grouped_entities=True)\n",
    "classifier(\"Hugging Face is a French company based in New York City.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7736791b-9585-4534-9efe-3fae3b7b6ca8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T09:14:15.118144300Z",
     "start_time": "2024-02-15T09:14:15.116181153Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "369dda97-bd1a-4cb0-9636-47c2308c6289",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "\n",
    "**Question Answering**(问答)是另一个token-level的任务，返回一个问题的答案，有时带有上下文（开放领域），有时不带上下文（封闭领域）。每当我们向虚拟助手提出问题时，例如询问一家餐厅是否营业，就会发生这种情况。它还可以提供客户或技术支持，并帮助搜索引擎检索您要求的相关信息。\n",
    "\n",
    "有两种常见的问答类型：\n",
    "\n",
    "- 提取式：给定一个问题和一些上下文，模型必须从上下文中提取出一段文字作为答案\n",
    "- 生成式：给定一个问题和一些上下文，答案是根据上下文生成的；这种方法由`Text2TextGenerationPipeline`处理，而不是下面展示的`QuestionAnsweringPipeline`\n",
    "\n",
    "模型主页：https://huggingface.co/distilbert-base-cased-distilled-squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a5281b0b-6b57-4884-92e1-cc2677987360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:56:38.290505453Z",
     "start_time": "2024-02-15T10:55:11.385516629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce88458a103f4339922895012ca82864"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a175ce382664474fbb3ef1e0fb064335"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0993be4a61f24ad0ab962342a3577e9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffb4b4cf67cb44dfb51ac605cffbb479"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f2141c2830b45bc9e3e326109452f21"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5ec8273f5a84a60a9af207d66a9bf48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# question_answerer = pipeline(task=\"question-answering\")\n",
    "question_answerer = pipeline(task=\"question-answering\", model=\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca674a51-30a4-4dea-a443-e428925e990f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:57:22.421734112Z",
     "start_time": "2024-02-15T10:57:21.054885728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9068, start: 30, end: 54, answer: huggingface/transformers\n"
     ]
    }
   ],
   "source": [
    "preds = question_answerer(\n",
    "    question=\"What is the name of the repository?\",\n",
    "    context=\"The name of the repository is huggingface/transformers\",\n",
    ")\n",
    "print(\n",
    "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f2158feb-a528-410a-aabf-f3bd7f2726c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:57:30.122149456Z",
     "start_time": "2024-02-15T10:57:28.284088240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.7504, start: 115, end: 122, answer: Beijing\n"
     ]
    }
   ],
   "source": [
    "preds = question_answerer(\n",
    "    question=\"What is the capital of China?\",\n",
    "    context=\"On 1 October 1949, CCP Chairman Mao Zedong formally proclaimed the People's Republic of China in Tiananmen Square, Beijing.\",\n",
    ")\n",
    "print(\n",
    "    f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fdb1d7e5-6baa-4142-81c1-5d311c0440e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:57:37.607597791Z",
     "start_time": "2024-02-15T10:57:37.605490352Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eebd7483-99a5-4f2a-894c-13cf5a3b71b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:57:38.701528898Z",
     "start_time": "2024-02-15T10:57:38.698562008Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d995cb85-ab8f-4b28-9413-1a83fa3e4c4d",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "**Summarization**(文本摘要）从较长的文本中创建一个较短的版本，同时尽可能保留原始文档的大部分含义。摘要是一个序列到序列的任务；它输出比输入更短的文本序列。有许多长篇文档可以进行摘要，以帮助读者快速了解主要要点。法案、法律和财务文件、专利和科学论文等文档可以摘要，以节省读者的时间并作为阅读辅助工具。\n",
    "\n",
    "与问答类似，摘要有两种类型：\n",
    "\n",
    "- 提取式：从原始文本中识别和提取最重要的句子\n",
    "- 生成式：从原始文本中生成目标摘要（可能包括输入文件中没有的新单词）；`SummarizationPipeline`使用生成式方法\n",
    "\n",
    "模型主页：https://huggingface.co/t5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e028f36-2b50-4ae3-8dce-2f0ac685e8fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T11:06:32.393282463Z",
     "start_time": "2024-02-15T11:01:56.116719315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ac93e7ae360419889b6c20f71e02562"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "025d0a1a0b0e482e9fef8e908dc3c587"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f841f195dca4ad0b6e606fc995935e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5b24b44d17845cdad3dd9ec3067cfc6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b23420d15254fb383c7048f9a5dcc16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39982b8bcb5e46b08bb59ccb41808e92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# summarizer = pipeline(task=\"summarization\", model=\"t5-base\", min_length=8, max_length=32)\n",
    "summarizer = pipeline(task=\"summarization\", model=\"facebook/bart-large-cnn\", min_length=8, max_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8c760bbf-2ae4-4f84-bd5e-32c30ee6bd78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T11:08:01.179978858Z",
     "start_time": "2024-02-15T11:06:56.818311439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[{'summary_text': 'The Transformer is the first sequence transduction model based entirely on attention. It replaces the recurrent layers most commonly used in encoder-decoder'}]"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\n",
    "    \"\"\"\n",
    "    In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, \n",
    "    replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. \n",
    "    For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. \n",
    "    On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. \n",
    "    In the former task our best model outperforms even all previously reported ensembles.\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b50ff52b-e61e-40cd-ab66-a591dc0c3b0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T11:09:17.374825745Z",
     "start_time": "2024-02-15T11:08:08.236277299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[{'summary_text': 'Transformers are a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. Unlike earlier recurrent neural'}]"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\n",
    "    '''\n",
    "    Large language models (LLM) are very large deep learning models that are pre-trained on vast amounts of data. \n",
    "    The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. \n",
    "    The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.\n",
    "    Transformer LLMs are capable of unsupervised training, although a more precise explanation is that transformers perform self-learning. \n",
    "    It is through this process that transformers learn to understand basic grammar, languages, and knowledge.\n",
    "    Unlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. \n",
    "    This allows the data scientists to use GPUs for training transformer-based LLMs, significantly reducing the training time.\n",
    "    '''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72315144-7fae-4848-af79-a70e428b2416",
   "metadata": {},
   "source": [
    "\n",
    "## Audio 音频处理任务\n",
    "\n",
    "音频和语音处理任务与其他模态略有不同，主要是因为音频作为输入是一个连续的信号。与文本不同，原始音频波形不能像句子可以被划分为单词那样被整齐地分割成离散的块。为了解决这个问题，通常在固定的时间间隔内对原始音频信号进行采样。如果在每个时间间隔内采样更多样本，采样率就会更高，音频更接近原始音频源。\n",
    "\n",
    "以前的方法是预处理音频以从中提取有用的特征。现在更常见的做法是直接将原始音频波形输入到特征编码器中，以提取音频表示。这样可以简化预处理步骤，并允许模型学习最重要的特征。\n",
    "\n",
    "### Audio classification\n",
    "\n",
    "**Audio classification**(音频分类)是一项将音频数据从预定义的类别集合中进行标记的任务。这是一个广泛的类别，具有许多具体的应用，其中一些包括：\n",
    "\n",
    "- 声学场景分类：使用场景标签（“办公室”、“海滩”、“体育场”）对音频进行标记。\n",
    "- 声学事件检测：使用声音事件标签（“汽车喇叭声”、“鲸鱼叫声”、“玻璃破碎声”）对音频进行标记。\n",
    "- 标记：对包含多种声音的音频进行标记（鸟鸣、会议中的说话人识别）。\n",
    "- 音乐分类：使用流派标签（“金属”、“嘻哈”、“乡村”）对音乐进行标记。\n",
    "\n",
    "模型主页：https://huggingface.co/superb/hubert-base-superb-er\n",
    "\n",
    "数据集主页：https://huggingface.co/datasets/superb#er\n",
    "\n",
    "```\n",
    "情感识别（ER）为每个话语预测一个情感类别。我们采用了最广泛使用的ER数据集IEMOCAP，并遵循传统的评估协议：我们删除不平衡的情感类别，只保留最后四个具有相似数量数据点的类别，并在标准分割的五折交叉验证上进行评估。评估指标是准确率（ACC）。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d4c1e9-8a67-4c72-8dce-ab326e0bc3b6",
   "metadata": {},
   "source": [
    "#### 前置依赖包安装\n",
    "\n",
    "建议在命令行安装必要的音频数据处理包: ffmpeg\n",
    "\n",
    "```shell\n",
    "$apt update & apt upgrade\n",
    "$apt install -y ffmpeg\n",
    "$pip install ffmpeg ffmpeg-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e1f8a8d-75eb-49ab-9353-6c9d1f384cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:30:04.189044051Z",
     "start_time": "2024-02-15T10:30:03.499255127Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at superb/hubert-base-superb-er were not used when initializing HubertForSequenceClassification: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at superb/hubert-base-superb-er and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# classifier = pipeline(task=\"audio-classification\", model=\"superb/hubert-base-superb-er\")\n",
    "classifier = pipeline(\"audio-classification\", model=\"superb/hubert-base-superb-ks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "204a4159-d14d-4623-8340-93d15abcc549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:30:08.323965436Z",
     "start_time": "2024-02-15T10:30:07.111599965Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg: error while loading shared libraries: libopenh264.so.5: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Malformed soundfile",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[57], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 使用 Hugging Face Datasets 上的测试文件\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[43mclassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhttps://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m preds \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mround\u001B[39m(pred[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m4\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m: pred[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]} \u001B[38;5;28;01mfor\u001B[39;00m pred \u001B[38;5;129;01min\u001B[39;00m preds]\n\u001B[1;32m      4\u001B[0m preds\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/audio_classification.py:136\u001B[0m, in \u001B[0;36mAudioClassificationPipeline.__call__\u001B[0;34m(self, inputs, **kwargs)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    105\u001B[0m     inputs: Union[np\u001B[38;5;241m.\u001B[39mndarray, \u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    107\u001B[0m ):\n\u001B[1;32m    108\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;124;03m    Classify the sequence(s) given as inputs. See the [`AutomaticSpeechRecognitionPipeline`] documentation for more\u001B[39;00m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;124;03m    information.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;124;03m        - **score** (`float`) -- The corresponding probability.\u001B[39;00m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/base.py:1162\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1155\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1156\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1159\u001B[0m         )\n\u001B[1;32m   1160\u001B[0m     )\n\u001B[1;32m   1161\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/base.py:1168\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1167\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[0;32m-> 1168\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1169\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[1;32m   1170\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/audio_classification.py:158\u001B[0m, in \u001B[0;36mAudioClassificationPipeline.preprocess\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    155\u001B[0m             inputs \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[0;32m--> 158\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[43mffmpeg_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_extractor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msampling_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;66;03m# Accepting `\"array\"` which is the key defined in `datasets` for\u001B[39;00m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# better integration\u001B[39;00m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msampling_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m inputs \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m inputs \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m inputs)):\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/audio_classification.py:62\u001B[0m, in \u001B[0;36mffmpeg_read\u001B[0;34m(bpayload, sampling_rate)\u001B[0m\n\u001B[1;32m     60\u001B[0m audio \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(out_bytes, np\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m audio\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMalformed soundfile\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m audio\n",
      "\u001B[0;31mValueError\u001B[0m: Malformed soundfile"
     ]
    }
   ],
   "source": [
    "# 使用 Hugging Face Datasets 上的测试文件\n",
    "preds = classifier(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2528bf81-8289-4bb9-bf2d-c0ce9f7e8b36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:30:10.040336861Z",
     "start_time": "2024-02-15T10:30:09.982730576Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg: error while loading shared libraries: libopenh264.so.5: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Malformed soundfile",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 使用本地的音频文件做测试\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[43mclassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata/audio/mlk.flac\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m preds \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mround\u001B[39m(pred[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;241m4\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m: pred[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]} \u001B[38;5;28;01mfor\u001B[39;00m pred \u001B[38;5;129;01min\u001B[39;00m preds]\n\u001B[1;32m      4\u001B[0m preds\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/audio_classification.py:136\u001B[0m, in \u001B[0;36mAudioClassificationPipeline.__call__\u001B[0;34m(self, inputs, **kwargs)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    105\u001B[0m     inputs: Union[np\u001B[38;5;241m.\u001B[39mndarray, \u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    107\u001B[0m ):\n\u001B[1;32m    108\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;124;03m    Classify the sequence(s) given as inputs. See the [`AutomaticSpeechRecognitionPipeline`] documentation for more\u001B[39;00m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;124;03m    information.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;124;03m        - **score** (`float`) -- The corresponding probability.\u001B[39;00m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/base.py:1162\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1155\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1156\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1159\u001B[0m         )\n\u001B[1;32m   1160\u001B[0m     )\n\u001B[1;32m   1161\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/base.py:1168\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1167\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[0;32m-> 1168\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1169\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[1;32m   1170\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/audio_classification.py:158\u001B[0m, in \u001B[0;36mAudioClassificationPipeline.preprocess\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    155\u001B[0m             inputs \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[0;32m--> 158\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[43mffmpeg_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_extractor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msampling_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;66;03m# Accepting `\"array\"` which is the key defined in `datasets` for\u001B[39;00m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# better integration\u001B[39;00m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msampling_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m inputs \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m inputs \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m inputs)):\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/audio_classification.py:62\u001B[0m, in \u001B[0;36mffmpeg_read\u001B[0;34m(bpayload, sampling_rate)\u001B[0m\n\u001B[1;32m     60\u001B[0m audio \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(out_bytes, np\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m audio\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMalformed soundfile\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m audio\n",
      "\u001B[0;31mValueError\u001B[0m: Malformed soundfile"
     ]
    }
   ],
   "source": [
    "# 使用本地的音频文件做测试\n",
    "preds = classifier(\"data/audio/mlk.flac\")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6826a83-f5a5-454e-ac16-1f215abef861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:13:37.566339340Z",
     "start_time": "2024-02-15T10:13:37.562154151Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e073cd1f-ba16-40a7-a029-bbd5e2a53dee",
   "metadata": {},
   "source": [
    "### Automatic speech recognition（ASR）\n",
    "\n",
    "**Automatic speech recognition**（自动语音识别）将语音转录为文本。这是最常见的音频任务之一，部分原因是因为语音是人类交流的自然形式。如今，ASR系统嵌入在智能技术产品中，如扬声器、电话和汽车。我们可以要求虚拟助手播放音乐、设置提醒和告诉我们天气。\n",
    "\n",
    "但是，Transformer架构帮助解决的一个关键挑战是低资源语言。通过在大量语音数据上进行预训练，仅在一个低资源语言的一小时标记语音数据上进行微调，仍然可以产生与以前在100倍更多标记数据上训练的ASR系统相比高质量的结果。\n",
    "\n",
    "模型主页：https://huggingface.co/openai/whisper-small\n",
    "\n",
    "下面展示使用 `OpenAI Whisper Small` 模型实现 ASR 的 Pipeline API 示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68d14f8c-1571-4889-abd4-8fde09dc610a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:16:35.703411814Z",
     "start_time": "2024-02-15T10:13:46.275606195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59cb9c4962504f00bc4bebd4ad3a8b30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81b65459eff446048073accd414951e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dcbfb4a2d4ac4cef8c6e801093fcda4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b849a1b048d1409da4d315805edc568e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3db7bbb74eb14582964930b10c9e2037"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b91c35aa2acb49bbbfda125dfd64719b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8286b343077b467dad652e17295c89ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d68c6dae65b9409b90f7ffff6a5c43da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36e3ce3272b447e99394ae3a737f854a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3561ed3b930e4fe2ba562fb774597442"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05458f0663614ff6aebccb0d6450c9e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 使用 `model` 参数指定模型\n",
    "transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "316ebe18-9c8e-4ded-96b7-751304aa9122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:16:40.560837847Z",
     "start_time": "2024-02-15T10:16:40.476984132Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg: error while loading shared libraries: libopenh264.so.5: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Soundfile is either not in the correct format or is malformed. Ensure that the soundfile has a valid audio file extension (e.g. wav, flac or mp3) and is not corrupted. If reading from a remote URL, ensure that the URL is the full address to **download** the audio file.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[43mtranscriber\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata/audio/mlk.flac\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m text\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py:292\u001B[0m, in \u001B[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001B[0;34m(self, inputs, **kwargs)\u001B[0m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    231\u001B[0m     inputs: Union[np\u001B[38;5;241m.\u001B[39mndarray, \u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    233\u001B[0m ):\n\u001B[1;32m    234\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001B[39;00m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;124;03m    documentation for more information.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    290\u001B[0m \u001B[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001B[39;00m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 292\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/base.py:1154\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001B[1;32m   1153\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ChunkPipeline):\n\u001B[0;32m-> 1154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1155\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1156\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[1;32m   1157\u001B[0m                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001B[1;32m   1158\u001B[0m             )\n\u001B[1;32m   1159\u001B[0m         )\n\u001B[1;32m   1160\u001B[0m     )\n\u001B[1;32m   1161\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001B[0m, in \u001B[0;36mPipelineIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_item()\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[0;32m--> 124\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator)\n\u001B[1;32m    125\u001B[0m processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(item, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[1;32m    126\u001B[0m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:266\u001B[0m, in \u001B[0;36mPipelinePackIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    263\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m accumulator\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_last:\n\u001B[0;32m--> 266\u001B[0m     processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[1;32m    267\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    268\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed, torch\u001B[38;5;241m.\u001B[39mTensor):\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:32\u001B[0m, in \u001B[0;36m_IterableDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index:\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 32\u001B[0m         data\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset_iter))\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m     34\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mended \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:183\u001B[0m, in \u001B[0;36mPipelineChunkIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubiterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;66;03m# Try to return next item\u001B[39;00m\n\u001B[0;32m--> 183\u001B[0m     processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubiterator)\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;66;03m# When a preprocess iterator ends, we can start lookig at the next item\u001B[39;00m\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001B[39;00m\n\u001B[1;32m    190\u001B[0m     \u001B[38;5;66;03m# into a single list, but with generators\u001B[39;00m\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubiterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py:369\u001B[0m, in \u001B[0;36mAutomaticSpeechRecognitionPipeline.preprocess\u001B[0;34m(self, inputs, chunk_length_s, stride_length_s)\u001B[0m\n\u001B[1;32m    366\u001B[0m             inputs \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m    368\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[0;32m--> 369\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[43mffmpeg_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_extractor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msampling_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    371\u001B[0m stride \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    372\u001B[0m extra \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/anaconda3/envs/transformers/lib/python3.11/site-packages/transformers/pipelines/audio_utils.py:41\u001B[0m, in \u001B[0;36mffmpeg_read\u001B[0;34m(bpayload, sampling_rate)\u001B[0m\n\u001B[1;32m     39\u001B[0m audio \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(out_bytes, np\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m audio\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 41\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSoundfile is either not in the correct format or is malformed. Ensure that the soundfile has \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma valid audio file extension (e.g. wav, flac or mp3) and is not corrupted. If reading from a remote \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     44\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mURL, ensure that the URL is the full address to **download** the audio file.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     45\u001B[0m     )\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m audio\n",
      "\u001B[0;31mValueError\u001B[0m: Soundfile is either not in the correct format or is malformed. Ensure that the soundfile has a valid audio file extension (e.g. wav, flac or mp3) and is not corrupted. If reading from a remote URL, ensure that the URL is the full address to **download** the audio file."
     ]
    }
   ],
   "source": [
    "text = transcriber(\"data/audio/mlk.flac\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d5e06-ac4a-4a56-be36-9e445751bda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "228b1482-1e13-4557-8d37-5d56e961b5c5",
   "metadata": {},
   "source": [
    "## Computer Vision 计算机视觉\n",
    "\n",
    "**Computer Vision**（计算机视觉）任务中最早成功之一是使用卷积神经网络（CNN）识别邮政编码数字图像。图像由像素组成，每个像素都有一个数值。这使得将图像表示为像素值矩阵变得容易。每个像素值组合描述了图像的颜色。\n",
    "\n",
    "计算机视觉任务可以通过以下两种通用方式解决：\n",
    "\n",
    "- 使用卷积来学习图像的层次特征，从低级特征到高级抽象特征。\n",
    "- 将图像分成块，并使用Transformer逐步学习每个图像块如何相互关联以形成图像。与CNN偏好的自底向上方法不同，这种方法有点像从一个模糊的图像开始，然后逐渐将其聚焦清晰。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bf9f4-6413-409b-968e-f03ca0c88367",
   "metadata": {},
   "source": [
    "### Image Classificaiton\n",
    "\n",
    "**Image Classificaiton**(图像分类)将整个图像从预定义的类别集合中进行标记。像大多数分类任务一样，图像分类有许多实际用例，其中一些包括：\n",
    "\n",
    "- 医疗保健：标记医学图像以检测疾病或监测患者健康状况\n",
    "- 环境：标记卫星图像以监测森林砍伐、提供野外管理信息或检测野火\n",
    "- 农业：标记农作物图像以监测植物健康或用于土地使用监测的卫星图像\n",
    "- 生态学：标记动物或植物物种的图像以监测野生动物种群或跟踪濒危物种\n",
    "\n",
    "模型主页：https://huggingface.co/google/vit-base-patch16-224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "98ca9736-10b9-45b0-a3e3-925f153bab57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T11:12:32.242148489Z",
     "start_time": "2024-02-15T11:12:25.430748185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/69.6k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4257ede0cfb4fb5a986ac049c1e7799"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f83bbda95c544361b86b0010d053054c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "preprocessor_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "038b0016c2c04fc4a8d9e40cbf4fa0b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# classifier = pipeline(task=\"image-classification\")\n",
    "classifier = pipeline(task=\"image-classification\", model=\"microsoft/resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8a41647-e8db-4d29-9dac-06c3c145acaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T11:12:39.195555946Z",
     "start_time": "2024-02-15T11:12:35.365061827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5583, 'label': 'lynx, catamount'}\n",
      "{'score': 0.1374, 'label': 'tabby, tabby cat'}\n",
      "{'score': 0.0822, 'label': 'marmot'}\n",
      "{'score': 0.043, 'label': 'badger'}\n",
      "{'score': 0.0139, 'label': 'Egyptian cat'}\n"
     ]
    }
   ],
   "source": [
    "preds = classifier(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ecb60-ebb9-4f4e-82bc-21e79bb22d90",
   "metadata": {},
   "source": [
    "![](data/image/cat-chonk.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "67592129-6430-4349-8016-7dde511ff69b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T11:12:47.565203223Z",
     "start_time": "2024-02-15T11:12:44.824462057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5583, 'label': 'lynx, catamount'}\n",
      "{'score': 0.1374, 'label': 'tabby, tabby cat'}\n",
      "{'score': 0.0822, 'label': 'marmot'}\n",
      "{'score': 0.043, 'label': 'badger'}\n",
      "{'score': 0.0139, 'label': 'Egyptian cat'}\n"
     ]
    }
   ],
   "source": [
    "# 使用本地图片（狼猫）\n",
    "preds = classifier(\n",
    "    \"data/image/cat-chonk.jpeg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8113c251-a648-4ea2-baa4-3081bb490c70",
   "metadata": {},
   "source": [
    "![](data/image/panda.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a7e9b51a-d759-4398-9894-f10933dbed47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T11:12:58.311684527Z",
     "start_time": "2024-02-15T11:12:55.440629691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9768, 'label': 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca'}\n",
      "{'score': 0.0088, 'label': 'indri, indris, Indri indri, Indri brevicaudatus'}\n",
      "{'score': 0.0004, 'label': 'groenendael'}\n",
      "{'score': 0.0003, 'label': 'Siberian husky'}\n",
      "{'score': 0.0002, 'label': 'malamute, malemute, Alaskan malamute'}\n"
     ]
    }
   ],
   "source": [
    "# 使用本地图片（熊猫）\n",
    "preds = classifier(\n",
    "    \"data/image/panda.jpg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "120f5fdb-2803-4f4f-9008-00dee9a6a557",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:19:29.326899480Z",
     "start_time": "2024-02-15T10:19:29.322986198Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8118a561-3dc9-4693-8c3b-3e29c37b4d71",
   "metadata": {},
   "source": [
    "### Object Detection\n",
    "\n",
    "与图像分类不同，目标检测在图像中识别多个对象以及这些对象在图像中的位置（由边界框定义）。目标检测的一些示例应用包括：\n",
    "\n",
    "- 自动驾驶车辆：检测日常交通对象，如其他车辆、行人和红绿灯\n",
    "- 遥感：灾害监测、城市规划和天气预报\n",
    "- 缺陷检测：检测建筑物中的裂缝或结构损坏，以及制造业产品缺陷\n",
    "\n",
    "模型主页：https://huggingface.co/facebook/detr-resnet-50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7122f04-7add-4623-8f3f-ccc00247524b",
   "metadata": {},
   "source": [
    "#### 前置依赖包安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8cbc4227-cbcc-4f18-bf26-66535e66afb6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-15T10:19:37.946344371Z",
     "start_time": "2024-02-15T10:19:36.590940124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Loading egg at /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages/huggingface_hub-0.20.3-py3.8.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: timm in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (0.9.12)\r\n",
      "Requirement already satisfied: torch>=1.7 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from timm) (2.2.0)\r\n",
      "Requirement already satisfied: torchvision in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from timm) (0.16.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from timm) (6.0.1)\r\n",
      "Requirement already satisfied: huggingface-hub in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages/huggingface_hub-0.20.3-py3.8.egg (from timm) (0.20.3)\r\n",
      "Requirement already satisfied: safetensors in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from timm) (0.4.2)\r\n",
      "Requirement already satisfied: filelock in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (1.12)\r\n",
      "Requirement already satisfied: networkx in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (2.8.8)\r\n",
      "Requirement already satisfied: jinja2 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (2023.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torch>=1.7->timm) (2.2.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7->timm) (12.3.101)\r\n",
      "Requirement already satisfied: requests in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from huggingface-hub->timm) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from huggingface-hub->timm) (4.66.2)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from huggingface-hub->timm) (23.2)\r\n",
      "Requirement already satisfied: numpy in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torchvision->timm) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from torchvision->timm) (9.4.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from jinja2->torch>=1.7->timm) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from requests->huggingface-hub->timm) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from requests->huggingface-hub->timm) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from requests->huggingface-hub->timm) (2.2.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from requests->huggingface-hub->timm) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ps/anaconda3/envs/transformers/lib/python3.11/site-packages (from sympy->torch>=1.7->timm) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c3401126-756b-4394-930c-c833c1f574b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T11:14:56.797107162Z",
     "start_time": "2024-02-15T11:14:43.170366776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6910072af3b1489481e1730c63a0c084"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/115M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ff79f4688f24824bf4f09894f41b52f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "764864c0561b40239c706c8765842073"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "preprocessor_config.json:   0%|          | 0.00/274 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "befb82a0d5394c5f8637cdb4622b004a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# detector = pipeline(task=\"object-detection\")\n",
    "detector = pipeline(task=\"object-detection\", model=\"microsoft/table-transformer-structure-recognition\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "32dc65d4-3868-4d10-a433-9f50832e8e68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T11:15:08.723897374Z",
     "start_time": "2024-02-15T11:15:02.031576787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': 0.9905,\n  'label': 'table',\n  'box': {'xmin': 66, 'ymin': 55, 'xmax': 881, 'ymax': 592}}]"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = detector(\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550759be-93ab-4988-81f0-c0cb8eccfda8",
   "metadata": {},
   "source": [
    "![](data/image/cat_dog.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "40f66cd7-f2c8-4af3-b9ab-60012792ec1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T11:15:21.385537852Z",
     "start_time": "2024-02-15T11:15:15.203402919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': 0.9968,\n  'label': 'table',\n  'box': {'xmin': 46, 'ymin': 33, 'xmax': 552, 'ymax': 369}}]"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = detector(\n",
    "    \"data/image/cat_dog.jpg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d591246-662b-47e0-8531-f862b05ba1ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:23:25.849779589Z",
     "start_time": "2024-02-15T10:23:25.847229063Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1565e1d-20bf-4b19-86c1-dd3a8983a999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T10:23:26.556417971Z",
     "start_time": "2024-02-15T10:23:26.549744318Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4993a9f-1032-46ab-97b1-a028d4bd5ebc",
   "metadata": {},
   "source": [
    "### Homework：替换以上示例中的模型，对比不同模型在相同任务上的性能表现\n",
    "\n",
    "在 Hugging Face Models 中找到适合你的模型：https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f51d2-863d-44f6-836f-9051351985ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
